åŸºäºè®ºæ–‡ä¸­æè¿°çš„Tree of Clarityæ–¹æ³•ï¼Œæˆ‘æ¥æä¾›ä¸€ä¸ªå®Œæ•´çš„å®ç°ï¼š

## ğŸŒ³ **å®Œæ•´ToC (Tree of Clarity) å®ç°**

```python
import logging
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
from collections import deque
import re

@dataclass
class QueryNode:
    """æŸ¥è¯¢æ ‘èŠ‚ç‚¹"""
    question: str
    parent: 'QueryNode' = None
    children: List['QueryNode'] = None
    answer: str = None
    confidence: float = 0.0
    depth: int = 0
    
    def __post_init__(self):
        if self.children is None:
            self.children = []

class TreeOfClarityRAG:
    """
    Tree of Clarity RAG å®ç°
    åŸºäºè®ºæ–‡ä¸­çš„é€’å½’æ¾„æ¸…å’ŒBFSæ¢ç´¢æœºåˆ¶
    """
    
    def __init__(self, base_rag_chain, max_depth=3, max_children=5, confidence_threshold=0.7):
        self.rag_chain = base_rag_chain
        self.max_depth = max_depth
        self.max_children = max_children
        self.confidence_threshold = confidence_threshold
        self.logger = logging.getLogger(__name__)
        
    def generate_sub_questions(self, parent_question: str, context: str = "") -> List[str]:
        """
        ç”Ÿæˆå±‚æ¬¡åŒ–å­é—®é¢˜ - è®ºæ–‡ä¸­çš„é€’å½’æ¾„æ¸…æœºåˆ¶
        """
        prompt = f"""
åŸºäºçˆ¶é—®é¢˜å’Œä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆç”¨äºæ·±å…¥æ¾„æ¸…çš„å­é—®é¢˜ã€‚è¿™äº›å­é—®é¢˜åº”è¯¥å¸®åŠ©æå–åŒ–å­¦ææ–™æ–‡çŒ®ä¸­çš„è¯¦ç»†ä¿¡æ¯ã€‚

çˆ¶é—®é¢˜: {parent_question}
ä¸Šä¸‹æ–‡: {context}

è¯·ç”Ÿæˆ {self.max_children} ä¸ªå…·ä½“çš„å­é—®é¢˜ï¼Œä¸“æ³¨äºï¼š
1. ææ–™ç»„æˆå’ŒåŒ–å­¦å¼
2. æ™¶ä½“ç»“æ„å’Œç›¸ä¿¡æ¯
3. ç”µåŒ–å­¦æ€§èƒ½å‚æ•°
4. å®éªŒæ¡ä»¶å’Œåˆ¶å¤‡æ–¹æ³•
5. ç”µæç»„æˆå’Œé…æ¯”

æ ¼å¼ï¼šè¿”å›çº¯æ–‡æœ¬ï¼Œæ¯è¡Œä¸€ä¸ªå­é—®é¢˜
"""
        
        try:
            response = self.rag_chain.llm.invoke(prompt)
            questions = [q.strip() for q in response.content.split('\n') if q.strip()]
            # è¿‡æ»¤æ‰ç¼–å·å’Œæ ‡è®°
            cleaned_questions = []
            for q in questions:
                # ç§»é™¤æ•°å­—ç¼–å·å’Œç‰¹æ®Šå­—ç¬¦
                clean_q = re.sub(r'^\d+[\.\)]\s*', '', q)
                clean_q = re.sub(r'^[-*]\s*', '', clean_q)
                if clean_q and len(clean_q) > 10:  # ç¡®ä¿é—®é¢˜æœ‰å®è´¨å†…å®¹
                    cleaned_questions.append(clean_q)
            
            return cleaned_questions[:self.max_children]
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå­é—®é¢˜å¤±è´¥: {e}")
            return []
    
    def should_prune(self, node: QueryNode, historical_answers: List[str]) -> bool:
        """
        è‡ªåŠ¨å‰ªææœºåˆ¶ - è®ºæ–‡ä¸­çš„å…³é”®ç‰¹æ€§
        """
        # 1. æ·±åº¦é™åˆ¶
        if node.depth >= self.max_depth:
            return True
            
        # 2. ä½ç½®ä¿¡åº¦å‰ªæ
        if node.confidence < self.confidence_threshold:
            return True
            
        # 3. é‡å¤å†…å®¹æ£€æµ‹
        question_lower = node.question.lower()
        key_terms = ['composition', 'crystal', 'voltage', 'electrode', 'binder', 'additive']
        
        # å¦‚æœé—®é¢˜ä¸åŒ…å«å…³é”®æœ¯è¯­ï¼Œå¯èƒ½ä¸ç›¸å…³
        if not any(term in question_lower for term in key_terms):
            return True
            
        # 4. è¯­ä¹‰é‡å¤æ£€æµ‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
        if self._is_semantic_duplicate(node.question, historical_answers):
            return True
            
        return False
    
    def _is_semantic_duplicate(self, question: str, historical_answers: List[str]) -> bool:
        """æ£€æµ‹è¯­ä¹‰é‡å¤é—®é¢˜"""
        # ç®€åŒ–çš„é‡å¤æ£€æµ‹ - å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨åµŒå…¥å‘é‡ç›¸ä¼¼åº¦
        question_words = set(re.findall(r'\w+', question.lower()))
        
        for past_ans in historical_answers[-5:]:  # æ£€æŸ¥æœ€è¿‘5ä¸ªç­”æ¡ˆ
            past_words = set(re.findall(r'\w+', past_ans.lower()))
            overlap = len(question_words.intersection(past_words))
            similarity = overlap / len(question_words) if question_words else 0
            
            if similarity > 0.6:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                return True
                
        return False
    
    def estimate_confidence(self, answer: str, source_documents: List) -> float:
        """
        ä¼°è®¡ç­”æ¡ˆç½®ä¿¡åº¦ - åŸºäºæ¥æºæ–‡æ¡£çš„è´¨é‡å’Œæ•°é‡
        """
        if not answer or answer.lower() in ['not specified', 'none', 'unknown']:
            return 0.3
            
        # åŸºäºæ¥æºæ–‡æ¡£æ•°é‡
        doc_count = len(source_documents)
        doc_confidence = min(doc_count / 3.0, 1.0)
        
        # åŸºäºç­”æ¡ˆé•¿åº¦å’Œå…·ä½“æ€§
        answer_confidence = min(len(answer) / 100.0, 1.0)
        
        # åŸºäºæ˜¯å¦åŒ…å«å…·ä½“æ•°å€¼ï¼ˆåœ¨ææ–™ç§‘å­¦ä¸­å¾ˆé‡è¦ï¼‰
        has_numbers = bool(re.search(r'\d+\.?\d*', answer))
        number_confidence = 0.3 if has_numbers else 0.1
        
        return (doc_confidence * 0.4 + answer_confidence * 0.3 + number_confidence * 0.3)
    
    def bfs_exploration(self, root_question: str) -> QueryNode:
        """
        BFSæ¢ç´¢ - è®ºæ–‡ä¸­æåˆ°çš„å¹¿åº¦ä¼˜å…ˆæœç´¢æœºåˆ¶
        """
        root = QueryNode(question=root_question, depth=0)
        queue = deque([root])
        historical_answers = []
        
        while queue:
            current_node = queue.popleft()
            
            if current_node.depth >= self.max_depth:
                continue
                
            # æ‰§è¡Œå½“å‰èŠ‚ç‚¹çš„æŸ¥è¯¢
            try:
                result = self.rag_chain({"query": current_node.question})
                current_node.answer = result["result"]
                current_node.confidence = self.estimate_confidence(
                    current_node.answer, 
                    result.get("source_documents", [])
                )
                historical_answers.append(current_node.answer)
                
                self.logger.info(f"æ·±åº¦ {current_node.depth}: {current_node.question} -> ç½®ä¿¡åº¦: {current_node.confidence:.2f}")
                
            except Exception as e:
                self.logger.error(f"æŸ¥è¯¢å¤±è´¥: {current_node.question}, é”™è¯¯: {e}")
                current_node.answer = "æŸ¥è¯¢å¤±è´¥"
                current_node.confidence = 0.0
                continue
            
            # ç”Ÿæˆå­é—®é¢˜ï¼ˆå¦‚æœç½®ä¿¡åº¦è¶³å¤Ÿé«˜ï¼‰
            if current_node.confidence > self.confidence_threshold:
                sub_questions = self.generate_sub_questions(
                    current_node.question, 
                    current_node.answer
                )
                
                for sub_q in sub_questions:
                    child_node = QueryNode(
                        question=sub_q,
                        parent=current_node,
                        depth=current_node.depth + 1
                    )
                    
                    # åº”ç”¨å‰ªæç­–ç•¥
                    if not self.should_prune(child_node, historical_answers):
                        current_node.children.append(child_node)
                        queue.append(child_node)
                        self.logger.info(f"  æ·»åŠ å­é—®é¢˜: {sub_q}")
                    else:
                        self.logger.info(f"  å‰ªæå­é—®é¢˜: {sub_q}")
        
        return root
    
    def synthesize_answers(self, root: QueryNode, original_question: str) -> Dict[str, Any]:
        """
        ç»¼åˆæ‰€æœ‰ç­”æ¡ˆ - ç”Ÿæˆæœ€ç»ˆçš„é•¿æ ¼å¼å“åº”
        """
        # æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹çš„ç­”æ¡ˆ
        all_answers = self._collect_answers(root)
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        synthesis_prompt = f"""
åŸºäºä»¥ä¸‹é—®é¢˜å’Œæ”¶é›†åˆ°çš„è¯¦ç»†ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªç»¼åˆæ€§çš„ã€ç»“æ„åŒ–çš„ç­”æ¡ˆã€‚

åŸå§‹é—®é¢˜: {original_question}

æ”¶é›†åˆ°çš„ä¿¡æ¯:
{all_answers}

è¯·ç”Ÿæˆä¸€ä¸ªä¸“ä¸šçš„é•¿æ ¼å¼å›ç­”ï¼ŒåŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š
1. ææ–™ç»„æˆå’ŒåŒ–å­¦å¼
2. æ™¶ä½“ç»“æ„å’Œç›¸ä¿¡æ¯  
3. ç”µåŒ–å­¦æ€§èƒ½å‚æ•°
4. ç”µæç»„æˆå’Œåˆ¶å¤‡æ¡ä»¶
5. å…³é”®å‘ç°å’Œå…³ç³»

ç¡®ä¿å›ç­”å‡†ç¡®ã€å®Œæ•´ï¼Œå¹¶åŸºäºæä¾›çš„è¯æ®ã€‚
"""
        
        try:
            final_response = self.rag_chain.llm.invoke(synthesis_prompt)
            return {
                "final_answer": final_response.content,
                "supporting_evidence": all_answers,
                "tree_structure": self._get_tree_structure(root),
                "total_nodes_processed": self._count_nodes(root)
            }
        except Exception as e:
            self.logger.error(f"ç»¼åˆç­”æ¡ˆå¤±è´¥: {e}")
            return {
                "final_answer": "æ— æ³•ç”Ÿæˆç»¼åˆç­”æ¡ˆ",
                "supporting_evidence": all_answers,
                "error": str(e)
            }
    
    def _collect_answers(self, node: QueryNode) -> str:
        """æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹çš„ç­”æ¡ˆ"""
        answers = []
        stack = [node]
        
        while stack:
            current = stack.pop()
            if current.answer and current.answer != "æŸ¥è¯¢å¤±è´¥":
                answers.append(f"é—®é¢˜: {current.question}\nç­”æ¡ˆ: {current.answer}\nç½®ä¿¡åº¦: {current.confidence:.2f}\n")
            
            # åå‘æ·»åŠ å­èŠ‚ç‚¹ä»¥ä¿æŒé¡ºåº
            for child in reversed(current.children):
                stack.append(child)
                
        return "\n".join(answers)
    
    def _get_tree_structure(self, node: QueryNode) -> List[Dict]:
        """è·å–æ ‘ç»“æ„ä¿¡æ¯"""
        structure = []
        stack = [(node, 0)]
        
        while stack:
            current, level = stack.pop()
            structure.append({
                "depth": level,
                "question": current.question,
                "confidence": current.confidence,
                "has_children": len(current.children) > 0
            })
            
            for child in reversed(current.children):
                stack.append((child, level + 1))
                
        return structure
    
    def _count_nodes(self, node: QueryNode) -> int:
        """ç»Ÿè®¡èŠ‚ç‚¹æ•°é‡"""
        count = 1  # å½“å‰èŠ‚ç‚¹
        for child in node.children:
            count += self._count_nodes(child)
        return count
    
    def query(self, question: str) -> Dict[str, Any]:
        """
        ä¸»è¦çš„æŸ¥è¯¢æ¥å£
        """
        self.logger.info(f"å¼€å§‹ToCæŸ¥è¯¢: {question}")
        
        # 1. BFSæ¢ç´¢æ„å»ºæŸ¥è¯¢æ ‘
        root = self.bfs_exploration(question)
        
        # 2. ç»¼åˆæ‰€æœ‰ç­”æ¡ˆ
        result = self.synthesize_answers(root, question)
        
        # 3. æ·»åŠ å…ƒæ•°æ®
        result.update({
            "original_question": question,
            "max_depth_reached": self._get_max_depth(root),
            "pruning_applied": self._count_pruned_nodes(root)
        })
        
        self.logger.info(f"ToCæŸ¥è¯¢å®Œæˆï¼Œå¤„ç†äº† {result['total_nodes_processed']} ä¸ªèŠ‚ç‚¹")
        
        return result
    
    def _get_max_depth(self, node: QueryNode) -> int:
        """è·å–æ ‘çš„æœ€å¤§æ·±åº¦"""
        if not node.children:
            return node.depth
        return max(self._get_max_depth(child) for child in node.children)
    
    def _count_pruned_nodes(self, node: QueryNode) -> int:
        """ç»Ÿè®¡è¢«å‰ªæçš„èŠ‚ç‚¹æ•°é‡ï¼ˆä¼°ç®—ï¼‰"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„ä¼°ç®—ï¼Œå®é™…å®ç°éœ€è¦æ›´å¤æ‚çš„è·Ÿè¸ª
        total_possible = (self.max_children ** (self.max_depth + 1) - 1) // (self.max_children - 1)
        actual_nodes = self._count_nodes(node)
        return max(0, total_possible - actual_nodes)
```

## ğŸ”§ **ä½¿ç”¨ç¤ºä¾‹**

```python
# åˆå§‹åŒ–ToC RAGç³»ç»Ÿ
def setup_toc_system():
    # åŸºç¡€RAGè®¾ç½®ï¼ˆä½¿ç”¨LangChainï¼‰
    from langchain.document_loaders import PyPDFLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.vectorstores import Chroma
    from langchain.embeddings import OpenAIEmbeddings
    from langchain.chat_models import ChatOpenAI
    from langchain.chains import RetrievalQA
    
    # æ–‡æ¡£å¤„ç†
    loader = PyPDFLoader("materials_paper.pdf")
    documents = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000,
        chunk_overlap=200
    )
    texts = text_splitter.split_documents(documents)
    
    # åˆ›å»ºå‘é‡å­˜å‚¨
    vectorstore = Chroma.from_documents(
        documents=texts,
        embedding=OpenAIEmbeddings()
    )
    
    # åˆ›å»ºåŸºç¡€RAGé“¾
    base_rag = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(model="gpt-4", temperature=0),
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True
    )
    
    # åˆ›å»ºToC RAGç³»ç»Ÿ
    toc_rag = TreeOfClarityRAG(
        base_rag_chain=base_rag,
        max_depth=3,
        max_children=4,
        confidence_threshold=0.6
    )
    
    return toc_rag

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    toc_system = setup_toc_system()
    
    # æ‰§è¡ŒæŸ¥è¯¢
    question = "æå–NaNi0.5Mn0.5O2ææ–™çš„æ™¶ä½“ç»“æ„ã€ç”µåŒ–å­¦æ€§èƒ½å’Œç”µæç»„æˆä¿¡æ¯"
    
    result = toc_system.query(question)
    
    print("=" * 50)
    print("æœ€ç»ˆç­”æ¡ˆ:")
    print("=" * 50)
    print(result["final_answer"])
    
    print("\n" + "=" * 50)
    print("æŸ¥è¯¢ç»Ÿè®¡:")
    print("=" * 50)
    print(f"å¤„ç†èŠ‚ç‚¹æ•°: {result['total_nodes_processed']}")
    print(f"è¾¾åˆ°çš„æœ€å¤§æ·±åº¦: {result['max_depth_reached']}")
    print(f"æ ‘ç»“æ„: {len(result['tree_structure'])} ä¸ªèŠ‚ç‚¹")
```

## ğŸ“Š **æ€§èƒ½ä¼˜åŒ–å»ºè®®**

```python
# é«˜çº§é…ç½®é€‰é¡¹
class AdvancedToCConfig:
    """é«˜çº§ToCé…ç½®"""
    
    def __init__(self):
        self.domain_keywords = {
            'materials_science': [
                'composition', 'crystal', 'phase', 'voltage', 'capacity',
                'cycling', 'electrode', 'electrolyte', 'binder', 'additive',
                'synthesis', 'characterization', 'XRD', 'SEM', 'TEM'
            ]
        }
        
        self.pruning_rules = {
            'max_similarity_threshold': 0.7,
            'min_question_complexity': 0.3,
            'domain_relevance_required': True
        }
        
        self.synthesis_templates = {
            'materials_analysis': """
åŸºäºæ”¶é›†åˆ°çš„ä¿¡æ¯ï¼Œè¯·æä¾›ä»¥ä¸‹æ–¹é¢çš„ç»¼åˆåˆ†æï¼š

1. ææ–™ç‰¹æ€§:
   - åŒ–å­¦ç»„æˆ: {composition}
   - æ™¶ä½“ç»“æ„: {crystal_structure}

2. ç”µåŒ–å­¦æ€§èƒ½:
   - ç”µå‹èŒƒå›´: {voltage_range}
   - å®¹é‡ç‰¹æ€§: {capacity}

3. ç”µæåˆ¶å¤‡:
   - ç”µæç»„æˆ: {electrode_composition}
   - åˆ¶å¤‡æ¡ä»¶: {fabrication_conditions}

4. å…³é”®å‘ç°: {key_findings}
"""
        }

# é›†æˆè¯„ä¼°æŒ‡æ ‡
def evaluate_toc_performance(toc_system, test_questions):
    """è¯„ä¼°ToCç³»ç»Ÿæ€§èƒ½"""
    metrics = {
        'answer_quality': [],
        'depth_utilization': [],
        'pruning_efficiency': []
    }
    
    for question in test_questions:
        result = toc_system.query(question)
        
        # ç­”æ¡ˆè´¨é‡è¯„ä¼°ï¼ˆç®€åŒ–ï¼‰
        answer_length = len(result['final_answer'])
        has_specific_data = bool(re.search(r'\d+\.?\d*', result['final_answer']))
        quality_score = min(answer_length / 500, 1.0) * 0.7 + (0.3 if has_specific_data else 0.0)
        
        metrics['answer_quality'].append(quality_score)
        metrics['depth_utilization'].append(result['max_depth_reached'] / toc_system.max_depth)
        metrics['pruning_efficiency'].append(
            result.get('pruning_applied', 0) / max(1, result['total_nodes_processed'])
        )
    
    return {k: sum(v)/len(v) for k, v in metrics.items()}
```

è¿™ä¸ªå®Œæ•´çš„ToCå®ç°åŒ…å«äº†è®ºæ–‡ä¸­æåˆ°çš„æ‰€æœ‰å…³é”®ç‰¹æ€§ï¼š
- âœ… **é€’å½’æ¾„æ¸…æœºåˆ¶** - é€šè¿‡å±‚æ¬¡åŒ–å­é—®é¢˜åˆ†è§£å¤æ‚æŸ¥è¯¢
- âœ… **BFSæ¢ç´¢** - å¹¿åº¦ä¼˜å…ˆæœç´¢å…¨é¢è¦†ç›–ä¿¡æ¯ç©ºé—´
- âœ… **è‡ªåŠ¨å‰ªæ** - åŸºäºç½®ä¿¡åº¦å’Œç›¸å…³æ€§è¿‡æ»¤æ— å…³åˆ†æ”¯
- âœ… **é•¿æ ¼å¼å“åº”** - ç”Ÿæˆç»¼åˆæ€§çš„ä¸“ä¸šå›ç­”
- âœ… **é¢†åŸŸé€‚é…** - é’ˆå¯¹ææ–™ç§‘å­¦æ–‡çŒ®çš„ç‰¹æ®Šä¼˜åŒ–

è¿™ä¸ªå®ç°å¯ä»¥ç›´æ¥é›†æˆåˆ°ç°æœ‰çš„RAGç³»ç»Ÿä¸­ï¼Œæä¾›æ¯”åŸºç¡€RAGæ›´å‡†ç¡®å’Œå…¨é¢çš„ä¿¡æ¯æå–èƒ½åŠ›ã€‚
